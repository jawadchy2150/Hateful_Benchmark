{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f836d6b8",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "This notebook implements an advanced benchmark of four distinct Vision Language Models (VLMs) on the Hateful Memes Challenge Dataset (HMCD). This version loads the dataset from local files, uses a balanced dataset sample, and evaluates models from four different families to capture classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f8235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be0f59",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset from Local Files & Prepare Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdeb5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = 'data' \n",
    "ANNOTATION_FILE = os.path.join(DATASET_FOLDER, 'dev.jsonl')\n",
    "IMG_DIR = os.path.join(DATASET_FOLDER, 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9c0cd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(ANNOTATION_FILE):\n",
    "    print(f\"Error'\")\n",
    "else:\n",
    "    df = pd.read_json(ANNOTATION_FILE, lines=True)\n",
    "    # Create the full path to each image file\n",
    "    df['img_path'] = df['img'].apply(lambda x: os.path.join(IMG_DIR, x))\n",
    "    print(\"Local dataset loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39640549",
   "metadata": {},
   "source": [
    "### Balanced Sampling Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9442261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_text'] = df['label'].map({0: 'non-hateful', 1: 'hateful'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a5aceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hateful_df = df[df['label'] == 1].sample(n=250, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97b0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hateful_df = df[df['label'] == 0].sample(n=250, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a660b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.concat([hateful_df, non_hateful_df])\n",
    "df_sample = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce05edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93541</td>\n",
       "      <td>img/93541.png</td>\n",
       "      <td>0</td>\n",
       "      <td>the jihad squad</td>\n",
       "      <td>data/img/img/93541.png</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83946</td>\n",
       "      <td>img/83946.png</td>\n",
       "      <td>1</td>\n",
       "      <td>when a white kid shoots up the school the same...</td>\n",
       "      <td>data/img/img/83946.png</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62703</td>\n",
       "      <td>img/62703.png</td>\n",
       "      <td>0</td>\n",
       "      <td>they shot the wrong gorilla</td>\n",
       "      <td>data/img/img/62703.png</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85761</td>\n",
       "      <td>img/85761.png</td>\n",
       "      <td>1</td>\n",
       "      <td>no wonder why the pipes are not working</td>\n",
       "      <td>data/img/img/85761.png</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39578</td>\n",
       "      <td>img/39578.png</td>\n",
       "      <td>1</td>\n",
       "      <td>one of the bullets they dug out of a victim fr...</td>\n",
       "      <td>data/img/img/39578.png</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  93541  img/93541.png      0   \n",
       "1  83946  img/83946.png      1   \n",
       "2  62703  img/62703.png      0   \n",
       "3  85761  img/85761.png      1   \n",
       "4  39578  img/39578.png      1   \n",
       "\n",
       "                                                text                img_path  \\\n",
       "0                                    the jihad squad  data/img/img/93541.png   \n",
       "1  when a white kid shoots up the school the same...  data/img/img/83946.png   \n",
       "2                        they shot the wrong gorilla  data/img/img/62703.png   \n",
       "3            no wonder why the pipes are not working  data/img/img/85761.png   \n",
       "4  one of the bullets they dug out of a victim fr...  data/img/img/39578.png   \n",
       "\n",
       "    label_text  \n",
       "0  non-hateful  \n",
       "1      hateful  \n",
       "2  non-hateful  \n",
       "3      hateful  \n",
       "4      hateful  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b52ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a balanced test set with 500 samples.\n",
      "Label distribution:\n",
      "label_text\n",
      "non-hateful    250\n",
      "hateful        250\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Created a balanced test set with {len(df_sample)} samples.\")\n",
    "print(\"Label distribution:\")\n",
    "print(df_sample['label_text'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5120da3",
   "metadata": {},
   "source": [
    "## Step 2: Playing with the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff1d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_shot_prompt():\n",
    "    \"\"\"Asks for a direct classification.\"\"\"\n",
    "    return (\n",
    "          \"You are an expert content moderation analyst. Your task is to determine if the provided meme is 'hateful' or 'non-hateful'. \"\n",
    "          \"After your classification, you MUST provide a single sentence justification for your decision. \"\n",
    "          \"Format your response as: 'CLASSIFICATION. Justification: YOUR_REASON_HERE.'\"\n",
    "          \"\\n\\nExample: 'hateful. Justification: The text uses a derogatory slur against a protected group.'\"\n",
    "          \"\\nExample: 'non-hateful. Justification: The meme is a harmless joke about animals and does not target any group.'\"\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64cb6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cot_prompt():\n",
    "    \"\"\"Guide the model to reason step-by-step before classifying.\"\"\"\n",
    "    return (\n",
    "        \"Perform the following steps:\\n\"\n",
    "        \"1. Briefly describe the visual elements in the image.\\n\"\n",
    "        \"2. Analyze the text on the meme.\\n\"\n",
    "        \"3. Consider the combined meaning of the visual elements and the text.\\n\\n\"\n",
    "        \"Based on your step-by-step analysis, classify the meme as 'hateful' or 'non-hateful'. \"\n",
    "        \"Your final response must be only the single word classification.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35f5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = create_zero_shot_prompt()\n",
    "# prompt_template = create_cot_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c4b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response_text):\n",
    "    cleaned_text = response_text.lower().strip()\n",
    "\n",
    "    classification = 'error'\n",
    "    justification = ''\n",
    "\n",
    "    if 'justification:' in cleaned_text:\n",
    "        try:\n",
    "            justification = cleaned_text.split('justification:')[1].strip()\n",
    "        except IndexError:\n",
    "            justification = \"Could not parse justification.\"\n",
    "\n",
    "    if cleaned_text.startswith('non-hateful'):\n",
    "        classification = 'non-hateful'\n",
    "    elif cleaned_text.startswith('hateful'):\n",
    "        classification = 'hateful'\n",
    "\n",
    "    return classification. justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42344692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_ollama(model_name, image_path, prompt):\n",
    "    try:\n",
    "        response = ollama.chat(model=model_name, messages=[{'role': 'user', 'content': prompt, 'images': [image_path]}])\n",
    "        return parse_response(response['message']['content'])\n",
    "    except Exception as e:\n",
    "        return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def image_to_base64(image_path):\n",
    "#     with open(image_path, \"rb\") as image_file:\n",
    "#         return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f90f6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9036d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmarking model: llava:7b ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing llava:7b: 100%|██████████| 250/250 [46:59<00:00, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmark Complete! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label_text</th>\n",
       "      <th>prediction_llava:7b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49360</td>\n",
       "      <td>non-hateful</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7198</td>\n",
       "      <td>hateful</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92738</td>\n",
       "      <td>hateful</td>\n",
       "      <td>error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50261</td>\n",
       "      <td>hateful</td>\n",
       "      <td>error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43175</td>\n",
       "      <td>hateful</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   label_text prediction_llava:7b\n",
       "0  49360  non-hateful         non-hateful\n",
       "1   7198      hateful             hateful\n",
       "2  92738      hateful               error\n",
       "3  50261      hateful               error\n",
       "4  43175      hateful         non-hateful"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_to_test = {\n",
    "    'llava:7b': classify_with_ollama\n",
    "}\n",
    "results_data = {model: [] for model in models_to_test}\n",
    "\n",
    "for model_name, classification_func in models_to_test.items():\n",
    "    print(f\"\\n--- Benchmarking model: {model_name} ---\")\n",
    "    for index, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=f\"Processing {model_name}\"):\n",
    "        if 'ollama' in classification_func.__name__:\n",
    "            pred, just = classification_func(model_name, row['img_path'], prompt_template)\n",
    "        else:\n",
    "            pred = classification_func(row['img_path'], prompt_template) ## for gemini or claude. will use later\n",
    "        results_data[model_name].append({'prediction': pred, 'justification': just})\n",
    "\n",
    "for model_name, data in results_data.items():\n",
    "    df_sample[f'prediction_{model_name}'] = [item['prediction'] for item in data]\n",
    "    df_sample[f'justification_{model_name}'] = [item['justification'] for item in data]\n",
    "\n",
    "print(\"\\n--- Benchmark Complete! ---\")\n",
    "display(df_sample[['id', 'label_text'] + [f'prediction_{model}' for model in models_to_test.keys()]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b88a7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_sample['label_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "926e12f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Report for: llava:7b ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     hateful       0.49      0.45      0.47       125\n",
      " non-hateful       0.51      0.38      0.44       125\n",
      "\n",
      "   micro avg       0.50      0.42      0.45       250\n",
      "   macro avg       0.50      0.42      0.45       250\n",
      "weighted avg       0.50      0.42      0.45       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for model_name in models_to_test.keys():\n",
    "    print(f\"\\n--- Evaluation Report for: {model_name} ---\")\n",
    "    model_predictions = df_sample[f'prediction_{model_name}']\n",
    "    report = classification_report(ground_truth, model_predictions, labels=['hateful', 'non-hateful'], zero_division=0)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a69f2d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_text</th>\n",
       "      <th>prediction_llava:7b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-hateful</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hateful</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hateful</td>\n",
       "      <td>error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hateful</td>\n",
       "      <td>error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hateful</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>hateful</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>hateful</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>hateful</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>non-hateful</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>hateful</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label_text prediction_llava:7b\n",
       "0    non-hateful         non-hateful\n",
       "1        hateful             hateful\n",
       "2        hateful               error\n",
       "3        hateful               error\n",
       "4        hateful         non-hateful\n",
       "..           ...                 ...\n",
       "245      hateful         non-hateful\n",
       "246      hateful         non-hateful\n",
       "247      hateful         non-hateful\n",
       "248  non-hateful         non-hateful\n",
       "249      hateful             hateful\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_show = ['label_text'] + [col for col in df_sample.columns if 'prediction' in col]\n",
    "full_results_df = df_sample[columns_to_show]\n",
    "display(full_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69940cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df.to_csv('full_benchmark_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
